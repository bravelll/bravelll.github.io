[{"categories":null,"contents":"大家好，很高兴你来我的博客，以下是我的一点介绍。\n我叫bravelll，目前从事架构设计开发工作，编程语言是golang，领域为云原生。\n上班用linux，有关的问题可以交流。\n喜欢golang,Ubuntu。\n我的博客园: https://bravelll.github.io/\n我的Github:https://github.com/bravelll\\\n","permalink":"https://bravelll.github.io/about/","tags":null,"title":"关于"},{"categories":["Flink"],"contents":"Flink CDC\u0026amp;SQL开发 基于1.14.x版本\n开发步骤 添加pom.xml \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.asiainfo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink14cdc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;name\u0026gt;flink14cdc\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.apache.org\u0026lt;/url\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;flink.version\u0026gt;1.14.6\u0026lt;/flink.version\u0026gt; \u0026lt;scala.binary.version\u0026gt;2.12\u0026lt;/scala.binary.version\u0026gt; \u0026lt;flink.cdc.version\u0026gt;2.3.0\u0026lt;/flink.cdc.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-java --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-streaming-java_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;!-- \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; --\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-clients_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-java-bridge_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-avro\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-kafka_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-jdbc_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-common\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.ververica\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-connector-mysql-cdc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.cdc.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.ververica\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-sql-connector-mysql-cdc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.cdc.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-runtime-web_${scala.binary.version}\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-planner_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;fastjson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.75\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/ru.yandex.clickhouse/clickhouse-jdbc --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;ru.yandex.clickhouse\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;clickhouse-jdbc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.code.gson\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;gson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.10.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.flink\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flink-table-api-scala_2.12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flink.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-assembly-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;descriptorRefs\u0026gt; \u0026lt;descriptorRef\u0026gt;jar-with-dependencies\u0026lt;/descriptorRef\u0026gt; \u0026lt;/descriptorRefs\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;make-assembly\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;single\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; FlinkSQL开发 package com.XXX.flink14cdc; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.table.api.bridge.java.StreamTableEnvironment; import org.apache.flink.table.api.Table; import org.apache.flink.types.Row; import org.apache.flink.streaming.api.datastream.DataStream; import java.time.ZoneId; import org.apache.flink.api.java.tuple.Tuple2; public class FlinkSQL { public static void main(String[] args) throws Exception { //1.执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.enableCheckpointing(3000); //env.setParallelism(1); StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env); String ddlsql=\u0026#34; CREATE TABLE user_source (\u0026#34; + \u0026#34; database_name STRING METADATA VIRTUAL,\u0026#34; + \u0026#34; table_name STRING METADATA VIRTUAL,\u0026#34; + \u0026#34; `id` DECIMAL(20, 0) NOT NULL,\u0026#34; + \u0026#34; name STRING,\u0026#34; + \u0026#34; address STRING,\u0026#34; + \u0026#34; phone_number STRING,\u0026#34; + \u0026#34; email STRING,\u0026#34; + \u0026#34; PRIMARY KEY (`id`) NOT ENFORCED\u0026#34; + \u0026#34; ) WITH (\u0026#34; + \u0026#34; \u0026#39;connector\u0026#39; = \u0026#39;mysql-cdc\u0026#39;,\u0026#34; + //\u0026#34; \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;initial\u0026#39;, \u0026#34;+ \u0026#34; \u0026#39;scan.startup.mode\u0026#39; = \u0026#39;latest-offset\u0026#39;, \u0026#34;+ \u0026#34; \u0026#39;server-time-zone\u0026#39; = \u0026#39;UTC\u0026#39;, \u0026#34;+ \u0026#34; \u0026#39;hostname\u0026#39; = \u0026#39;10.1.3.21\u0026#39;,\u0026#34; + \u0026#34; \u0026#39;port\u0026#39; = \u0026#39;3306\u0026#39;,\u0026#34; + \u0026#34; \u0026#39;username\u0026#39; = \u0026#39;root\u0026#39;,\u0026#34; + \u0026#34; \u0026#39;password\u0026#39; = \u0026#39;123456\u0026#39;,\u0026#34; + \u0026#34; \u0026#39;database-name\u0026#39; = \u0026#39;db_[0-9]+\u0026#39;,\u0026#34; + \u0026#34; \u0026#39;table-name\u0026#39; = \u0026#39;user_[0-9]+\u0026#39;\u0026#34; + \u0026#34; )\u0026#34;; tableEnv.executeSql(ddlsql); //Table table= tableEnv.sqlQuery(\u0026#34;select * from user_info\u0026#34;); Table table= tableEnv.sqlQuery(\u0026#34;select * from user_source\u0026#34;); tableEnv.toChangelogStream(table).print(); env.execute(\u0026#34;flinkSQLCDC\u0026#34;); } } FlinkCDC KAFKA 开发 package com.XXX.flink14cdc; import com.XXX.flink14cdc.func.ClickhouseSink; import com.XXX.flink14cdc.serialization.CustomerDeserializationSchema; import com.ververica.cdc.connectors.mysql.source.MySqlSource; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.connector.kafka.sink.KafkaRecordSerializationSchema; import org.apache.flink.connector.kafka.sink.KafkaSink; import org.apache.flink.runtime.state.hashmap.HashMapStateBackend; import org.apache.flink.runtime.state.storage.JobManagerCheckpointStorage; import org.apache.flink.streaming.api.CheckpointingMode; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.kafka.common.serialization.StringSerializer; public class FlinkKafka { public static void main(String[] args) throws Exception { //1.获取flink 执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); //2.启动检查点 env.enableCheckpointing(1000); env.getCheckpointConfig().setCheckpointTimeout(10000); env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); // 设置使用 HashMapStateBackend，Task State 存储于 TaskManager 堆内存中 env.setStateBackend(new HashMapStateBackend()); // 这样设置 checkpoint 的 state 存储方式：把 job State 存储在 JobManager 的堆内存中 env.getCheckpointConfig().setCheckpointStorage(new JobManagerCheckpointStorage()); // 需要设置外部高可用文件系统存储路径用来保存 Job State //env.getCheckpointConfig().setCheckpointStorage(\u0026#34;hdfs://hadoop10/flink/checkpoints\u0026#34;); MySqlSource\u0026lt;String\u0026gt; mySqlSource = MySqlSource.\u0026lt;String\u0026gt;builder().hostname(\u0026#34;10.1.3.75\u0026#34;).port(30306) .databaseList(\u0026#34;eseap\u0026#34;) // set captured database .tableList(\u0026#34;eseap.user_info\u0026#34;) // set captured table .username(\u0026#34;root\u0026#34;).password(\u0026#34;123456\u0026#34;) // .scanNewlyAddedTableEnabled(true) .deserializer(new CustomerDeserializationSchema()) // converts SourceRecord to JString //.deserializer(new MyJsonDebeziumDeserializationSchema()) // converts SourceRecord to JString //.startupOptions(StartupOptions.initial()) //.startupOptions(StartupOptions.earliest()) // .includeSchemaChanges(true) .serverTimeZone(\u0026#34;UTC\u0026#34;).serverId(\u0026#34;5300\u0026#34;).build(); DataStreamSource\u0026lt;String\u0026gt; streamSource = env.fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), \u0026#34;mysql-source\u0026#34;); streamSource.setParallelism(1); //打印 streamSource.print(); //下沉kafka KafkaSink\u0026lt;String\u0026gt; sink = KafkaSink.\u0026lt;String\u0026gt;builder().setBootstrapServers(\u0026#34;10.1.3.21:9092\u0026#34;) //.setTransactionalIdPrefix(\u0026#34;my-trx-id-prefix\u0026#34;) .setRecordSerializer(KafkaRecordSerializationSchema.builder() .setTopic(\u0026#34;usertopic\u0026#34;) .setKafkaKeySerializer(StringSerializer.class) .setKafkaValueSerializer(StringSerializer.class) .build()) .build(); //下沉kafka streamSource.sinkTo(sink); //启动任务 env.execute(\u0026#34;mysql-to-kafka-cdc\u0026#34;); } } FlinkCDC ClickHouse 开发 package com.xxx.flink14cdc; import com.xxx.flink14cdc.func.ClickhouseSink; import com.xxx.flink14cdc.serialization.CustomerDeserializationSchema; import com.ververica.cdc.connectors.mysql.source.MySqlSource; import org.apache.flink.api.common.eventtime.WatermarkStrategy; import org.apache.flink.runtime.state.hashmap.HashMapStateBackend; import org.apache.flink.runtime.state.storage.JobManagerCheckpointStorage; import org.apache.flink.streaming.api.CheckpointingMode; import org.apache.flink.streaming.api.datastream.DataStreamSource; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; public class FlinkCK { public static void main(String[] args) throws Exception { //1.获取flink 执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); //2.启动检查点 env.enableCheckpointing(1000); env.getCheckpointConfig().setCheckpointTimeout(10000); env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); env.getCheckpointConfig().setMaxConcurrentCheckpoints(1); // 设置使用 HashMapStateBackend，Task State 存储于 TaskManager 堆内存中 env.setStateBackend(new HashMapStateBackend()); // 这样设置 checkpoint 的 state 存储方式：把 job State 存储在 JobManager 的堆内存中 env.getCheckpointConfig().setCheckpointStorage(new JobManagerCheckpointStorage()); // 需要设置外部高可用文件系统存储路径用来保存 Job State //env.getCheckpointConfig().setCheckpointStorage(\u0026#34;hdfs://hadoop10/flink/checkpoints\u0026#34;); MySqlSource\u0026lt;String\u0026gt; mySqlSource = MySqlSource.\u0026lt;String\u0026gt;builder().hostname(\u0026#34;10.1.3.75\u0026#34;).port(30306) .databaseList(\u0026#34;eseap\u0026#34;) // set captured database .tableList(\u0026#34;eseap.user_info\u0026#34;) // set captured table .username(\u0026#34;root\u0026#34;).password(\u0026#34;123456\u0026#34;) // .scanNewlyAddedTableEnabled(true) .deserializer(new CustomerDeserializationSchema()) // converts SourceRecord to JString //.deserializer(new MyJsonDebeziumDeserializationSchema()) // converts SourceRecord to JString //.startupOptions(StartupOptions.initial()) //.startupOptions(StartupOptions.earliest()) // .includeSchemaChanges(true) .serverTimeZone(\u0026#34;UTC\u0026#34;).serverId(\u0026#34;5300\u0026#34;).build(); DataStreamSource\u0026lt;String\u0026gt; streamSource = env.fromSource(mySqlSource, WatermarkStrategy.noWatermarks(), \u0026#34;mysql-source\u0026#34;); streamSource.setParallelism(1); //打印 streamSource.print(); //下沉ck streamSource.addSink(new ClickhouseSink()); //启动任务 env.execute(\u0026#34;mysql-to-ck-cdc\u0026#34;); } } 自定义下沉组件ClickhouseSink 开发 import com.google.gson.Gson; import com.google.gson.internal.LinkedTreeMap; import org.apache.flink.configuration.Configuration; import org.apache.flink.streaming.api.functions.sink.RichSinkFunction; import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; import java.util.HashMap; public class ClickhouseSink extends RichSinkFunction\u0026lt;String\u0026gt; { Connection connection; PreparedStatement pstmt; String insertsql = \u0026#34;insert into sink_ck_user_info(id,name,sex) values (?,?,?)\u0026#34;; String deletesql=\u0026#34;ALTER TABLE sink_ck_user_info DELETE WHERE id = ?\u0026#34;; String updatesql=\u0026#34;ALTER TABLE sink_ck_user_info UPDATE name = ?, sex=? WHERE id=?\u0026#34;; private Connection getConnection() { Connection conn = null; try { Class.forName(\u0026#34;ru.yandex.clickhouse.ClickHouseDriver\u0026#34;); String url = \u0026#34;jdbc:clickhouse://10.1.3.71:8123/default\u0026#34;; conn = DriverManager.getConnection(url,\u0026#34;default\u0026#34;,\u0026#34;123456\u0026#34;); } catch (Exception e) { e.printStackTrace(); } return conn; } @Override public void open(Configuration parameters) throws Exception { super.open(parameters); connection = getConnection(); //insertpstmt = connection.prepareStatement(insertsql); } //每条记录插入时调用一次 @Override public void invoke(String value, Context context) throws Exception { //{\u0026#34;op\u0026#34;:\u0026#34;CREATE\u0026#34;,\u0026#34;before\u0026#34;:{},\u0026#34;after\u0026#34;:{\u0026#34;sex\u0026#34;:\u0026#34;male\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;liuys5\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;5\u0026#34;},\u0026#34;db\u0026#34;:\u0026#34;eseap\u0026#34;,\u0026#34;tableName\u0026#34;:\u0026#34;user_info\u0026#34;} Gson t = new Gson(); HashMap\u0026lt;String,Object\u0026gt; hs = t.fromJson(value,HashMap.class); String database = (String)hs.get(\u0026#34;db\u0026#34;); String table = (String)hs.get(\u0026#34;tableName\u0026#34;); String op = (String)hs.get(\u0026#34;op\u0026#34;); if(\u0026#34;eseap\u0026#34;.equals(database) \u0026amp;\u0026amp; \u0026#34;user_info\u0026#34;.equals(table)){ if(\u0026#34;CREATE\u0026#34;.equals(op)){ System.out.println(\u0026#34;insert =\u0026gt; \u0026#34;+value); LinkedTreeMap\u0026lt;String,Object\u0026gt; data = (LinkedTreeMap\u0026lt;String,Object\u0026gt;)hs.get(\u0026#34;after\u0026#34;); String name = (String)data.get(\u0026#34;name\u0026#34;); String sex = (String)data.get(\u0026#34;sex\u0026#34;); String id = (String)data.get(\u0026#34;id\u0026#34;); // 未前面的占位符赋值 pstmt = connection.prepareStatement(insertsql); pstmt.setString(1, id); pstmt.setString(2, name); pstmt.setString(3, sex); pstmt.executeUpdate(); } else if (\u0026#34;DELETE\u0026#34;.equals(op)) { System.out.println(\u0026#34;delete =\u0026gt; \u0026#34;+value); LinkedTreeMap\u0026lt;String,Object\u0026gt; data = (LinkedTreeMap\u0026lt;String,Object\u0026gt;)hs.get(\u0026#34;before\u0026#34;); String id = (String)data.get(\u0026#34;id\u0026#34;); // 未前面的占位符赋值 pstmt = connection.prepareStatement(deletesql); pstmt.setString(1, id); pstmt.executeUpdate(); } else if (\u0026#34;UPDATE\u0026#34;.equals(op)) { System.out.println(\u0026#34;update =\u0026gt; \u0026#34;+value); LinkedTreeMap\u0026lt;String,Object\u0026gt; data = (LinkedTreeMap\u0026lt;String,Object\u0026gt;)hs.get(\u0026#34;after\u0026#34;); String name = (String)data.get(\u0026#34;name\u0026#34;); String sex = (String)data.get(\u0026#34;sex\u0026#34;); String id = (String)data.get(\u0026#34;id\u0026#34;); // 未前面的占位符赋值 pstmt = connection.prepareStatement(updatesql); pstmt.setString(1, name); pstmt.setString(2, sex); pstmt.setString(3, id); pstmt.executeUpdate(); } } } @Override public void close() throws Exception { super.close(); if(pstmt != null) { pstmt.close(); } if(connection != null) { connection.close(); } } } 自定义序列化组件CustomerDeserializationSchema package com.xxx.flink14cdc.serialization; import org.apache.flink.api.common.typeinfo.TypeInformation; import org.apache.flink.util.Collector; import com.ververica.cdc.connectors.shaded.org.apache.kafka.connect.source.SourceRecord; import com.ververica.cdc.debezium.DebeziumDeserializationSchema; import java.util.List; import org.apache.flink.api.common.typeinfo.BasicTypeInfo; import com.ververica.cdc.connectors.shaded.org.apache.kafka.connect.data.Field; import com.ververica.cdc.connectors.shaded.org.apache.kafka.connect.data.Schema; import com.ververica.cdc.connectors.shaded.org.apache.kafka.connect.data.Struct; import com.alibaba.fastjson.JSONObject; import io.debezium.data.Envelope; public class CustomerDeserializationSchema implements DebeziumDeserializationSchema\u0026lt;String\u0026gt; { /** * */ private static final long serialVersionUID = 1L; @Override public TypeInformation\u0026lt;String\u0026gt; getProducedType() { return BasicTypeInfo.STRING_TYPE_INFO; } /** * { * \u0026#34;db\u0026#34;:\u0026#34;\u0026#34;, * \u0026#34;tablename\u0026#34;:\u0026#34;\u0026#34; * \u0026#34;before\u0026#34;:{\u0026#34;id\u0026#34;:\u0026#34;001\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;\u0026#34; * \u0026#34;after\u0026#34;:{\u0026#34;id\u0026#34;:\u0026#34;001\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;\u0026#34; * \u0026#34;op\u0026#34;:\u0026#34;\u0026#34; * } */ @Override public void deserialize(SourceRecord sourceRecord, Collector\u0026lt;String\u0026gt; collector) throws Exception { //創建json對象封裝數據 JSONObject result=new JSONObject(); //獲取數據庫和表名 String topic =sourceRecord.topic(); String[] fields=topic.split(\u0026#34;\\\\.\u0026#34;); result.put(\u0026#34;db\u0026#34;, fields[1]); result.put(\u0026#34;tableName\u0026#34;, fields[2]); //before數據 Struct value=(Struct)sourceRecord.value(); Struct before=value.getStruct(\u0026#34;before\u0026#34;); JSONObject beforeJson=new JSONObject(); if (before !=null) { //获取列信息 Schema schema =before.schema(); List\u0026lt;Field\u0026gt; fieldList=schema.fields(); for (Field field:fieldList) { beforeJson.put(field.name(), before.get(field)); }\t} result.put(\u0026#34;before\u0026#34;, beforeJson); //after 數據 Struct after=value.getStruct(\u0026#34;after\u0026#34;); JSONObject afterJson=new JSONObject(); if (after !=null) { //獲取列信息 Schema schema =after.schema(); List\u0026lt;Field\u0026gt; fieldList=schema.fields(); for (Field field:fieldList) { afterJson.put(field.name(), after.get(field)); }\t} result.put(\u0026#34;after\u0026#34;, afterJson); //獲取操作類型 Envelope.Operation operation=Envelope.operationFor(sourceRecord); result.put(\u0026#34;op\u0026#34;, operation); //輸出數據 collector.collect(result.toJSONString()); } } ","permalink":"https://bravelll.github.io/post/flinkcdcsql-api-%E5%BC%80%E5%8F%91/","tags":["FlinkCDC","FlinkSQL"],"title":"FlinkCDC\u0026SQL API 开发"},{"categories":["operator","kubebuilder"],"contents":"使用kubebuilder开发kubernetes核心资源的webhook\nkubernetes 的准入控制器的开发，通常情况下要么使用 go 的 net/http 开发http 服务端来实现逻辑，更简单的方法是使用 conntronl-runtime 实现。\n我们用 kubebuilder 来开发 k8s operator 的时候，可以生成自定义资源的 webhook ，同时也能方便的生成资源清单。但无论是 kubebuilder 还是 operator-sdk 框架都无法实现 core type 资源的 webhook 。kubebuiler 的官方文档提到了 core type webbook 的开发，但并未具体提供具体的开发指导。\n那么如何利用 kubebuiler 来简化核心资源准入控制器的开发？下面就探索了一种方式。\nkubebuilder webhook开发步骤 开发前准备\nkubebuilder 3.9\ngo 1.20.1\nkubectl 1.26.1\n生成代码框架\n#创建文件夹 mkdir deployment-webhook cd deployment-webhook/ #生成代码框架 go mod init github.com/bravelll/deployment-webhook kubebuilder init --domain bravelll.github.io --license none --owner \u0026#34;bravelll\u0026#34; #生成api，这里注意，我们要创建的是核心资源的 webhook，所以不需要生成 Resource (n) 和 需要Controller(n) kubebuilder create api --group apps --version v1 --kind Deployment 生成webhook\nkubebuilder create webhook --group apps --version v1 --kind Deployment --defaulting --programmatic-validation make manifests 修改文件deployment_webhook.go //伪代码 type Deployment struct { *appsv1.Deployment `json:\u0026#34;,inline\u0026#34;` } 修改配置文件config/default/kustomization.yaml bases: - ../crd - ../rbac - ../manager # [WEBHOOK] To enable webhook, uncomment all the sections with [WEBHOOK] prefix including the one in # crd/kustomization.yaml - ../webhook # [CERTMANAGER] To enable cert-manager, uncomment all sections with \u0026#39;CERTMANAGER\u0026#39;. \u0026#39;WEBHOOK\u0026#39; components are required. - ../certmanager # [PROMETHEUS] To enable prometheus monitor, uncomment all sections with \u0026#39;PROMETHEUS\u0026#39;. #- ../prometheus patchesStrategicMerge: # Protect the /metrics endpoint by putting it behind auth. # If you want your controller-manager to expose the /metrics # endpoint w/o any authn/z, please comment the following line. - manager_auth_proxy_patch.yaml # [WEBHOOK] To enable webhook, uncomment all the sections with [WEBHOOK] prefix including the one in # crd/kustomization.yaml #- manager_webhook_patch.yaml # [CERTMANAGER] To enable cert-manager, uncomment all sections with \u0026#39;CERTMANAGER\u0026#39;. # Uncomment \u0026#39;CERTMANAGER\u0026#39; sections in crd/kustomization.yaml to enable the CA injection in the admission webhooks. # \u0026#39;CERTMANAGER\u0026#39; needs to be enabled to use ca injection #- webhookcainjection_patch.yaml # the following config is for teaching kustomize how to do var substitution vars: # [CERTMANAGER] To enable cert-manager, uncomment all sections with \u0026#39;CERTMANAGER\u0026#39; prefix. - name: CERTIFICATE_NAMESPACE # namespace of the certificate CR objref: kind: Certificate group: cert-manager.io version: v1 name: serving-cert # this name should match the one in certificate.yaml fieldref: fieldpath: metadata.namespace - name: CERTIFICATE_NAME objref: kind: Certificate group: cert-manager.io version: v1 name: serving-cert # this name should match the one in certificate.yaml - name: SERVICE_NAMESPACE # namespace of the service objref: kind: Service version: v1 name: webhook-service fieldref: fieldpath: metadata.namespace - name: SERVICE_NAME objref: kind: Service version: v1 name: webhook-service 运行 make make install make run 引用相关文章 \u0026lt; https://songjlg.github.io/2021/10/19/使用kubebuilder开发kubernetes核心资源的webhook/\u0026gt; https://developer.aliyun.com/article/878545\n","permalink":"https://bravelll.github.io/post/%E4%BD%BF%E7%94%A8kubebuilder%E5%BC%80%E5%8F%91kubernetes%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E4%BA%8C/","tags":["kubebuilder","webhook"],"title":"使用kubebuilder开发kubernetes核心资源(二)"},{"categories":["operator","kubebuilder"],"contents":"使用kubebuilder开发kubernetes核心资源的controller\nkubebuilder开发步骤 开发前准备\nkubebuilder 3.9\ngo 1.20.1\nkubectl 1.26.1\n生成代码框架\n#创建文件夹 mkdir service-controller cd service-controller/ #生成代码框架 go mod init github.com/bravelll/service_controller kubebuilder init --domain bravelll.github.io --license none --owner \u0026#34;bravelll\u0026#34; #生成api，这里注意，我们要创建的是核心资源的 controller，所以不需要生成 Resource (n) 和 需要Controller(y) kubebuilder create api --group core --version v1 --kind Service Create Resource [y/n] n Create Controller [y/n] y 实现代码逻辑 修改service_controller.go\nfunc (r *ServiceReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { //为了测试用的 log := log.FromContext(ctx).WithName(\u0026#34;controllers\u0026#34;).WithName(\u0026#34;redisCluster\u0026#34;) instance := \u0026amp;corev1.Service{} //首先我们获取Service实例 err := r.Get(ctx, req.NamespacedName, instance) if err != nil { // 如果 Service 是被删除的，那么我们应该忽略掉 if errors.IsNotFound(err) { log.Info(\u0026#34;instance not found, maybe removed\u0026#34;) return ctrl.Result{}, nil } return ctrl.Result{}, err } log.Info(\u0026#34;Service变化了\u0026#34; + instance.Name) return ctrl.Result{}, nil } func (r *ServiceReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). WithEventFilter(\u0026amp;ResourceChangedPredicate{}). WithOptions(runtimecontroller.Options{MaxConcurrentReconciles: 2}). For(\u0026amp;corev1.Service{}). Complete(r) } 添加controller过滤器\npackage controllers import ( \u0026#34;sigs.k8s.io/controller-runtime/pkg/event\u0026#34; \u0026#34;sigs.k8s.io/controller-runtime/pkg/predicate\u0026#34; ) type ResourceChangedPredicate struct { predicate.Funcs } func (rl *ResourceChangedPredicate) Update(e event.UpdateEvent) bool { change := e.ObjectNew.GetAnnotations()[\u0026#34;test\u0026#34;] if change != \u0026#34;\u0026#34; \u0026amp;\u0026amp; change == \u0026#34;test\u0026#34; { return true } return false } func (rl *ResourceChangedPredicate) Create(e event.CreateEvent) bool { change := e.Object.GetAnnotations()[\u0026#34;test\u0026#34;] if change != \u0026#34;\u0026#34; \u0026amp;\u0026amp; change == \u0026#34;test\u0026#34; { return true } return false } // Delete returns true if the Delete event should be processed func (rl *ResourceChangedPredicate) Delete(e event.DeleteEvent) bool { return true } // Generic returns true if the Generic event should be processed func (rl *ResourceChangedPredicate) Generic(e event.GenericEvent) bool { return true } 实现代码逻辑\nmake make install make run ","permalink":"https://bravelll.github.io/post/%E4%BD%BF%E7%94%A8kubebuilder%E5%BC%80%E5%8F%91kubernetes%E6%A0%B8%E5%BF%83%E8%B5%84%E6%BA%90%E4%B8%80/","tags":["kubebuilder","controller"],"title":"使用kubebuilder开发kubernetes核心资源(一)"},{"categories":["bigdata","hadoop"],"contents":"hadoop集群基于docker-compose快速部署\n1.安装docker compose(基于docker 插件的方式安装) mkdir -p ~/.docker/cli-plugins/ curl -SL https://github.com/docker/compose/releases/download/v2.3.3/docker-compose-linux-x86_64 -o ~/.docker/cli-plugins/docker-compose chmod +x ~/.docker/cli-plugins/docker-compose docker compose version 2.克隆代码 git clone https://github.com/big-data-europe/docker-hadoop.git cd docker-hadoop 3.修改文件hadoop.env CORE_CONF_fs_defaultFS=hdfs://namenode:9000 CORE_CONF_hadoop_http_staticuser_user=root CORE_CONF_hadoop_proxyuser_hue_hosts=* CORE_CONF_hadoop_proxyuser_hue_groups=* CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec HDFS_CONF_dfs_webhdfs_enabled=true HDFS_CONF_dfs_permissions_enabled=false HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false #start ** 修改部分 ** CORE_CONF_hadoop_http_cross___origin_enabled=true CORE_CONF_hadoop_http_cross___origin_allowed___origins=* CORE_CONF_hadoop_http_cross___origin_allowed___methods=GET,POST,HEAD,DELETE,OPTIONS CORE_CONF_hadoop_http_cross___origin_allowed___headers=X-Requested-With,Content-Type,Accept,Origin CORE_CONF_hadoop_http_cross___origin_maxvage=1800 CORE_CONF_org_apache_hadoop_security_HttpCrossOriginFilterInitializer=hadoop.http.filter.initializers #end YARN_CONF_yarn_log___aggregation___enable=true YARN_CONF_yarn_log_server_url=http://historyserver:8188/applicationhistory/logs/ YARN_CONF_yarn_resourcemanager_recovery_enabled=true YARN_CONF_yarn_resourcemanager_store_class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore YARN_CONF_yarn_resourcemanager_scheduler_class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___mb=8192 YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___vcores=4 YARN_CONF_yarn_resourcemanager_fs_state___store_uri=/rmstate YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled=true YARN_CONF_yarn_resourcemanager_hostname=resourcemanager YARN_CONF_yarn_resourcemanager_address=resourcemanager:8032 YARN_CONF_yarn_resourcemanager_scheduler_address=resourcemanager:8030 YARN_CONF_yarn_resourcemanager_resource__tracker_address=resourcemanager:8031 YARN_CONF_yarn_timeline___service_enabled=true YARN_CONF_yarn_timeline___service_generic___application___history_enabled=true YARN_CONF_yarn_timeline___service_hostname=historyserver YARN_CONF_mapreduce_map_output_compress=true YARN_CONF_mapred_map_output_compress_codec=org.apache.hadoop.io.compress.SnappyCodec YARN_CONF_yarn_nodemanager_resource_memory___mb=16384 YARN_CONF_yarn_nodemanager_resource_cpu___vcores=8 YARN_CONF_yarn_nodemanager_disk___health___checker_max___disk___utilization___per___disk___percentage=98.5 YARN_CONF_yarn_nodemanager_remote___app___log___dir=/app-logs YARN_CONF_yarn_nodemanager_aux___services=mapreduce_shuffle MAPRED_CONF_mapreduce_framework_name=yarn MAPRED_CONF_mapred_child_java_opts=-Xmx4096m MAPRED_CONF_mapreduce_map_memory_mb=4096 MAPRED_CONF_mapreduce_reduce_memory_mb=8192 MAPRED_CONF_mapreduce_map_java_opts=-Xmx3072m MAPRED_CONF_mapreduce_reduce_java_opts=-Xmx6144m MAPRED_CONF_yarn_app_mapreduce_am_env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/ MAPRED_CONF_mapreduce_map_env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/ MAPRED_CONF_mapreduce_reduce_env=HADOOP_MAPRED_HOME=/opt/hadoop-3.2.1/ 4.修改文件docker-compose.yml 加入主机名hostname和datanode暴露端口给宿主机\nversion: \u0026#34;3\u0026#34; services: namenode: image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8 container_name: namenode hostname: namenode restart: always ports: - 9870:9870 - 9000:9000 volumes: - hadoop_namenode:/hadoop/dfs/name environment: - CLUSTER_NAME=hadoopcluster env_file: - ./hadoop.env datanode: image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8 container_name: datanode hostname: datanode #修改 restart: always volumes: - hadoop_datanode:/hadoop/dfs/data environment: SERVICE_PRECONDITION: \u0026#34;namenode:9870\u0026#34; env_file: - ./hadoop.env ports: - 9864:9864 ##修改 resourcemanager: image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8 container_name: resourcemanager hostname: resourcemanager restart: always environment: SERVICE_PRECONDITION: \u0026#34;namenode:9000 namenode:9870 datanode:9864\u0026#34; env_file: - ./hadoop.env nodemanager1: image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8 container_name: nodemanager hostname: nodemanager restart: always environment: SERVICE_PRECONDITION: \u0026#34;namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088\u0026#34; env_file: - ./hadoop.env historyserver: image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8 container_name: historyserver hostname: historyserver restart: always environment: SERVICE_PRECONDITION: \u0026#34;namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088\u0026#34; volumes: - hadoop_historyserver:/hadoop/yarn/timeline env_file: - ./hadoop.env volumes: hadoop_namenode: hadoop_datanode: hadoop_historyserver: 5.启停 docker compose up -d #启动 docker compose stop #停止 docker compose down #删除 6.web端访问 直接通过宿主机IP+对于服务器端口即可访问 这里注意下：在hdfs页面查看文件内容、下载文件，上传文件，需要在本地window的hosts文件添加以下数据节点映射地址 hosts文件路径：C:\\Windows\\System32\\drivers\\etc\n在末尾添加以下内容：\n宿主机ip nodemanager 宿主机ip historyserver 宿主机ip namenode 宿主机ip resourcemanager 宿主机ip datanode ","permalink":"https://bravelll.github.io/post/hadoop-%E5%9F%BA%E4%BA%8Edocker-compose%E9%83%A8%E7%BD%B2/","tags":["bigdata","docker"],"title":"hadoop-基于docker-compose部署"},{"categories":["Etcd","Election"],"contents":"通过etcd实现的分布式选举\n分布式选主 分布式锁可以保证当有多台实例同时竞争一把锁时，只有一个人会成功，其他的都是失败。诸如共享资源修改、幂等、频控等场景都可以通过分布式锁来实现。 还有一种场景，也可以通过分布式锁来实现，那就是选主，为了保证服务的可用性，我们都会以一主多从的方式去部署，特别是提供存储能力的服务。Leader服务来接收数据的写入，然后将数据同步给Follower服务。当Leader服务挂掉时，我们需要从Follower服务中重新选举一个服务来当Leader，复杂的方式是通过Raft协议去协商，简单点，可以通过分布式锁的思路来做：\n所有的Follower服务去竞争同一把锁，并给这个锁设置一个过期时间 只会有一个Follower服务取到锁，这把锁的值就为它的标识，他就变成了Leader服务 其他Follower服务竞争失败后，去获取锁得到的当前的Leader服务标识，与之通信 Leader服务需要在锁过期之前不断的续期，证明自己是健康的 所有Follower服务监控这把锁是否还被Leader服务持有，如果没有，就跳到了第1步\n通过 Redis、Zookeeper 都可以实现，不过这次，我们使用 Etcd 来实现。\n准备 启动etcd docker run -d --name Etcd-server \\ --publish 2379:2379 \\ --publish 2380:2380 \\ --env ALLOW_NONE_AUTHENTICATION=yes \\ --env ETCD_ADVERTISE_CLIENT_URLS=http://etcd-server:2379 \\ bitnami/etcd:latest golang 依赖库安装 go get go.etcd.io/etcd/client/v3 选主 example package main import ( \u0026#34;context\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;time\u0026#34; clientv3 \u0026#34;go.etcd.io/etcd/client/v3\u0026#34; \u0026#34;go.etcd.io/etcd/client/v3/concurrency\u0026#34; ) var ( serverName = flag.String(\u0026#34;name\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) ) func main() { flag.Parse() // Etcd 服务器地址 endpoints := []string{\u0026#34;10.1.3.54:2379\u0026#34;} clientConfig := clientv3.Config{ Endpoints: endpoints, DialTimeout: 2 * time.Second, } cli, err := clientv3.New(clientConfig) if err != nil { panic(err) } s1, err := concurrency.NewSession(cli) if err != nil { panic(err) } fmt.Println(\u0026#34;session lessId is \u0026#34;, s1.Lease()) e1 := concurrency.NewElection(s1, \u0026#34;my-election\u0026#34;) go func() { // 参与选举，如果选举成功，会定时续期 if err := e1.Campaign(context.Background(), *serverName); err != nil { fmt.Println(err) } }() masterName := \u0026#34;\u0026#34; go func() { ctx, cancel := context.WithCancel(context.TODO()) defer cancel() timer := time.NewTicker(time.Second) for range timer.C { timer.Reset(time.Second) select { case resp := \u0026lt;-e1.Observe(ctx): if len(resp.Kvs) \u0026gt; 0 { // 查看当前谁是 master masterName = string(resp.Kvs[0].Value) fmt.Println(\u0026#34;get master with:\u0026#34;, masterName) } } } }() go func() { timer := time.NewTicker(5 * time.Second) for range timer.C { // 判断自己是 master 还是 slave if masterName == *serverName { fmt.Println(\u0026#34;oh, i\u0026#39;m master\u0026#34;) } else { fmt.Println(\u0026#34;slave!\u0026#34;) } } }() c := make(chan os.Signal, 1) // 接收 Ctrl C 中断 signal.Notify(c, os.Interrupt, os.Kill) s := \u0026lt;-c fmt.Println(\u0026#34;Got signal:\u0026#34;, s) e1.Resign(context.TODO()) } 终端运行 go run main.go -name A go run main.go -name B 当我们使用 Ctrl C 中断A，此时 B 就成为了 master\n原理 当我们启动 A 和 B 两个服务时，他们后会在公共前缀 \u0026ldquo;my-election/\u0026rdquo; 下创建自己的 key，这个 key 的构成为 \u0026ldquo;my-election/\u0026rdquo; + 十六进制(LessId)。这个LessId 是在服务启动时，从 Etcd 服务端取到的客户端唯一标识。比如上面程序运行的两个服务创建的 key 分别是：\nA 服务创建的 key 是 \u0026ldquo;my-election/694d81e5fc652594\u0026rdquo;，值是 \u0026ldquo;A\u0026rdquo; B 服务创建的 key 是 \u0026ldquo;my-election/694d81e5fc65259c\u0026rdquo;，值是 \u0026ldquo;B\u0026rdquo; 因为是通过事务的方式去创建 key，可以保证如果这个 key 已经创建了，不去创建了。并且这个 key 是有过期时间，两个服务 A 和 B 会启动一个协程定期去刷新过期时间，通过这个方式证明自己的健康的。 现在两个服务都创建了 key， 那么那个才是 master 呢？我们选取最早创建的那个 key 的拥有者作为 master。 Etcd 服务的查询接口支持根据前缀查询和按照创建时间排序，所以我们可以轻松的拿到第一个创建成功的 key，这个 key 对应的值就是 master 了，也就是 A 服务。 当现在 master 服务挂掉了，因为它的 key 没有在过期之前续期，就会被删除的，此时当初第二个创建的 key 就会变成第一个，那个 master 就变成了 B 服务。\n我们是通过e1.Campaign(context.Background(), *serverName)行代码是参加去参加选举的，里面有一个细节：如果竞争失败，这个函数会阻塞，直到它选举成功或者服务中断。也就是说：如果 B 服务创建的 key 不是最早的一个，那它会一直等待，直到服务 A 的 key 被删除后，函数才会有返回。\n引用链接 https://juejin.cn/post/7118746548812382238\n","permalink":"https://bravelll.github.io/post/etcd%E5%88%86%E5%B8%83%E5%BC%8F%E9%80%89%E4%B8%BE/","tags":["golang"],"title":"Etcd分布式选举"},{"categories":["index"],"contents":"通过容器代理可以实现翻墙外国网站\n通过容器代理可以实现翻墙外国网站 通过修改相关配置实现containerd和dockerd的代理.\ncontainerd 实现步骤:\nmkdir /etc/systemd/system/containerd.service.d/ cat /etc/systemd/system/containerd.service.d/http-proxy.conf [Service] Environment=\u0026#34;HTTP_PROXY=http://10.1.1.12:8118\u0026#34; Environment=\u0026#34;HTTPS_PROXY=http://10.1.1.12:8118\u0026#34; Environment=\u0026#34;NO_PROXY=localhost,127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local\u0026#34; systemctl daemon-reload;systemctl restart containerd.service dockerd 实现步骤:\nmkdir -p /etc/systemd/system/docker.service.d cat /etc/systemd/system/docker.service.d/http-proxy.conf [Service] Environment=\u0026#34;HTTP_PROXY=http://10.1.1.12:8118\u0026#34; Environment=\u0026#34;HTTPS_PROXY=http://10.1.1.12:8118\u0026#34; Environment=\u0026#34;NO_PROXY=localhost,127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local\u0026#34; systemctl daemon-reload;systemctl restart docker chatGPT ChatGPT（全名：Chat Generative Pre-trained Transformer[2]）是由OpenAI开发的一个人工智能聊天机器人程序，于2022年11月推出。该程序使用基于GPT-3.5架构的大型语言模型并通过强化学习进行训练。\nChatGPT目前仍以文字方式互动，而除了可以透过人类自然对话方式进行交互，还可以用于相对复杂的语言工作，包括自动文本生成、自动问答、自动摘要等在内的多种任务。如：在自动文本生成方面，ChatGPT可以根据输入的文本自动生成类似的文本（剧本、歌曲、企划等），在自动问答方面，ChatGPT可以根据输入的问题自动生成答案。还具有编写和调试计算机程序的能力。[3]在推广期间，所有人可以免费注册，并在登入后免费使用ChatGPT实现与AI机器人对话[4]。\nChatGPT可以写出相似于真人程度的文章，并因其在许多知识领域给出详细的回答和清晰的答案而迅速获得关注，证明了从前认为不会被AI取代的知识型工作它也足以胜任，对于金融与白领人力市场的冲击相当大，但其事实准确性参差不齐被认为是一重大缺陷，其基于意识形态的模型训练结果并被认为需要小心地校正[5][6]。ChatGPT于2022年11月发布后，OpenAI估值已涨至290亿美元[7]。上线两个月后，用户数量达到1亿[8]。\n最新chatGPT论文\n","permalink":"https://bravelll.github.io/post/%E5%AE%B9%E5%99%A8%E4%BB%A3%E7%90%86/","tags":["tag-1","tag-2","tag-3"],"title":"容器代理"},{"categories":null,"contents":"","permalink":"https://bravelll.github.io/search/","tags":null,"title":"Search Results"}]